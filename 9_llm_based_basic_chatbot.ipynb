{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17cb85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from typing import TypedDict,Annotated\n",
    "from langchain_core.messages import HumanMessage,BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99ec5336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "870b5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatOpenAI(model=\"openai/gpt-4.1-nano\",base_url=\"https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f105461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot(TypedDict):\n",
    "    message:Annotated[list[BaseMessage],add_messages] #add_message is reducer fn like operator.add but is buit in langgraph for spec. purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44645837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state:ChatBot)->ChatBot:\n",
    "    messages=state[\"message\"]\n",
    "    state[\"message\"]=[model.invoke(messages)]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13a00e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7a5e2fc432f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint=MemorySaver()\n",
    "\n",
    "graph = StateGraph(ChatBot)\n",
    "\n",
    "graph.add_node(\"chat_node\",chat_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d46c5f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7a5e2fc432f0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START,\"chat_node\")\n",
    "graph.add_edge(\"chat_node\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c66f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow=graph.compile(checkpointer=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd1b76b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7a5e48f9b530>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f815582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_state={\n",
    "    \"message\":[HumanMessage(content=\"What is the capital of isle of man\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f287ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the capital of isle of man', additional_kwargs={}, response_metadata={}, id='2a28aafc-ae62-4165-8e16-d0eb09a0ed24'),\n",
       " AIMessage(content='The capital of the Isle of Man is Douglas.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 15, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 5.9e-06, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 5.9e-06, 'upstream_inference_prompt_cost': 1.5e-06, 'upstream_inference_completions_cost': 4.4e-06}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-4.1-nano', 'system_fingerprint': None, 'id': 'gen-1768579150-U5PWzAwIl0CqZSC8nBu7', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc788-3480-7423-8bf9-9909435a26c0-0', usage_metadata={'input_tokens': 15, 'output_tokens': 11, 'total_tokens': 26, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.invoke(inital_state)[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70590390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:  hi i am avnish\n",
      "AI: Hi Avnish! How can I assist you today?\n",
      "user:  what is my name\n",
      "AI: Your name is Avnish. How can I help you today?\n",
      "user:  What is Rl\n",
      "AI: \"RL\" can stand for different things depending on the context. Some common meanings include:\n",
      "\n",
      "1. **Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by receiving rewards or penalties.\n",
      "2. **Real Life**: Referring to everyday life situations.\n",
      "3. **Round Length**: Used in sports or gaming contexts.\n",
      "4. **Rate Limit**: In networking or API contexts, referring to restrictions on the number of requests.\n",
      "\n",
      "Could you please specify the context in which you're referring to \"RL\"? That way, I can provide a more accurate explanation.\n",
      "user:  where was it started\n",
      "AI: If you're referring to **Reinforcement Learning (RL)**, it has its roots in the early 20th century with foundational ideas in behaviorist psychology, but as a formal field of study, it was developed in the mid-20th century.\n",
      "\n",
      "**Key milestones in the development of Reinforcement Learning:**\n",
      "\n",
      "- **1950s-1960s:** The origins of RL are linked to research in psychology, particularly Edward Thorndike's Law of Effect, which suggested that behaviors followed by pleasant outcomes are likely to be repeated.\n",
      "- **1960s:** The formal mathematical framework for RL was developed. Richard S. Sutton and Andrew G. Barto are often credited with pioneering work in this area.\n",
      "- **1980s:** The field gained significant momentum with algorithms like Q-learning (introduced by Watkins in 1989) and Temporal Difference learning.\n",
      "- **1990s onward:** RL became central to advancements in artificial intelligence, notably in game playing and robotics.\n",
      "\n",
      "**In summary**, RL as a formal discipline was primarily developed in North America by researchers like Richard Sutton, and it has grown through contributions from computer scientists, psychologists, and mathematicians.\n",
      "\n",
      "Would you like more detailed history or examples?\n",
      "user:  can you write a simple rl min max algo\n",
      "AI: Certainly! Here's a simple example of a min-max algorithm, which is commonly used in two-player, zero-sum games like Tic-Tac-Toe. Just to clarify, min-max algorithms are not exactly reinforcement learning algorithms, but rather decision-tree search algorithms.\n",
      "\n",
      "### Simple Min-Max Algorithm for Tic-Tac-Toe (Python)\n",
      "\n",
      "```python\n",
      "# Define the board and players\n",
      "# 'X' is maximizing player, 'O' is minimizing player\n",
      "\n",
      "def is_moves_left(board):\n",
      "    return any(cell == '_' for row in board for cell in row)\n",
      "\n",
      "def evaluate(board):\n",
      "    # Check rows\n",
      "    for row in board:\n",
      "        if row.count('X') == 3:\n",
      "            return 10\n",
      "        elif row.count('O') == 3:\n",
      "            return -10\n",
      "    # Check columns\n",
      "    for col in range(3):\n",
      "        column = [board[row][col] for row in range(3)]\n",
      "        if column.count('X') == 3:\n",
      "            return 10\n",
      "        elif column.count('O') == 3:\n",
      "            return -10\n",
      "    # Check diagonals\n",
      "    diag1 = [board[i][i] for i in range(3)]\n",
      "    diag2 = [board[i][2 - i] for i in range(3)]\n",
      "    if diag1.count('X') == 3 or diag2.count('X') == 3:\n",
      "        return 10\n",
      "    if diag1.count('O') == 3 or diag2.count('O') == 3:\n",
      "        return -10\n",
      "    # No winner\n",
      "    return 0\n",
      "\n",
      "def minmax(board, depth, is_maximizing):\n",
      "    score = evaluate(board)\n",
      "    if score == 10 or score == -10:\n",
      "        return score\n",
      "    if not is_moves_left(board):\n",
      "        return 0  # Draw\n",
      "\n",
      "    if is_maximizing:\n",
      "        best_score = -float('inf')\n",
      "        for i in range(3):\n",
      "            for j in range(3):\n",
      "                if board[i][j] == '_':\n",
      "                    board[i][j] = 'X'\n",
      "                    current_score = minmax(board, depth + 1, False)\n",
      "                    board[i][j] = '_'\n",
      "                    best_score = max(best_score, current_score)\n",
      "        return best_score\n",
      "    else:\n",
      "        best_score = float('inf')\n",
      "        for i in range(3):\n",
      "            for j in range(3):\n",
      "                if board[i][j] == '_':\n",
      "                    board[i][j] = 'O'\n",
      "                    current_score = minmax(board, depth + 1, True)\n",
      "                    board[i][j] = '_'\n",
      "                    best_score = min(best_score, current_score)\n",
      "        return best_score\n",
      "\n",
      "# Example usage:\n",
      "initial_board = [\n",
      "    ['X', 'O', 'X'],\n",
      "    ['_', '_', '_'],\n",
      "    ['_', '_', '_']\n",
      "]\n",
      "\n",
      "best_score = -float('inf')\n",
      "best_move = None\n",
      "\n",
      "for i in range(3):\n",
      "    for j in range(3):\n",
      "        if initial_board[i][j] == '_':\n",
      "            initial_board[i][j] = 'X'\n",
      "            move_score = minmax(initial_board, 0, False)\n",
      "            initial_board[i][j] = '_'\n",
      "            if move_score > best_score:\n",
      "                best_score = move_score\n",
      "                best_move = (i, j)\n",
      "\n",
      "print(f\"Best move for X: {best_move} with score {best_score}\")\n",
      "```\n",
      "\n",
      "### What this code does:\n",
      "- It evaluates the current state of the Tic-Tac-Toe board.\n",
      "- Uses the min-max algorithm recursively to explore possible future moves.\n",
      "- Finds the optimal move for the maximum player ('X') assuming the opponent ('O') also plays optimally.\n",
      "\n",
      "**Note:** This is a simplified example suitable for small games like Tic-Tac-Toe. For larger games, you may need to incorporate optimizations like alpha-beta pruning.\n",
      "\n",
      "Would you like this explained further or adapted for a different game?\n",
      "user:  exit\n",
      "bye bye!\n"
     ]
    }
   ],
   "source": [
    "thread_id=\"1\"\n",
    "\n",
    "while(True):\n",
    "    user_message = input(\"type here\")\n",
    "    print(\"user: \",user_message)\n",
    "    if user_message.strip().lower() in [\"exit\",\"bye\",\"quit\"]:\n",
    "        print(\"bye bye!\")\n",
    "        break\n",
    "    config={\"configurable\":{\"thread_id\":thread_id}}\n",
    "    res=workflow.invoke({\"message\":[HumanMessage(content=user_message)]},config=config)\n",
    "    print(\"AI:\",res[\"message\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eeda68b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'message': [HumanMessage(content='hi i am avnish', additional_kwargs={}, response_metadata={}, id='ca345b53-cc55-4bd8-85e8-4f6af754a12f'), AIMessage(content='Hi Avnish! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 6.5e-06, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 6.5e-06, 'upstream_inference_prompt_cost': 1.3e-06, 'upstream_inference_completions_cost': 5.2e-06}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-4.1-nano', 'system_fingerprint': None, 'id': 'gen-1768581686-tWnmBjIYMA97ZCXHcEvf', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc7ae-e3fe-7e12-b4a2-7a04ffcd2617-0', usage_metadata={'input_tokens': 13, 'output_tokens': 13, 'total_tokens': 26, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}, id='9141d19c-c97b-4b5c-afb7-8a8a67978a66'), AIMessage(content='Your name is Avnish. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 37, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 9.7e-06, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 9.7e-06, 'upstream_inference_prompt_cost': 3.7e-06, 'upstream_inference_completions_cost': 6e-06}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-4.1-nano', 'system_fingerprint': None, 'id': 'gen-1768581696-EWP5gienMno2sfBC9WfX', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc7af-097f-7942-b061-1b9b00200c11-0', usage_metadata={'input_tokens': 37, 'output_tokens': 15, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='What is Rl', additional_kwargs={}, response_metadata={}, id='c893a59e-65fb-4985-a585-46ce3ac694c2'), AIMessage(content='\"RL\" can stand for different things depending on the context. Some common meanings include:\\n\\n1. **Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by receiving rewards or penalties.\\n2. **Real Life**: Referring to everyday life situations.\\n3. **Round Length**: Used in sports or gaming contexts.\\n4. **Rate Limit**: In networking or API contexts, referring to restrictions on the number of requests.\\n\\nCould you please specify the context in which you\\'re referring to \"RL\"? That way, I can provide a more accurate explanation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 63, 'total_tokens': 184, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 5.47e-05, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 5.47e-05, 'upstream_inference_prompt_cost': 6.3e-06, 'upstream_inference_completions_cost': 4.84e-05}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-4.1-nano', 'system_fingerprint': None, 'id': 'gen-1768581705-EDarWRRRe2UVg9Kxc7m5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc7af-2e0f-7842-97a6-e1fa7eb4a158-0', usage_metadata={'input_tokens': 63, 'output_tokens': 121, 'total_tokens': 184, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='where was it started', additional_kwargs={}, response_metadata={}, id='6e628eab-399d-4dda-9e82-133573669f52'), AIMessage(content=\"If you're referring to **Reinforcement Learning (RL)**, it has its roots in the early 20th century with foundational ideas in behaviorist psychology, but as a formal field of study, it was developed in the mid-20th century.\\n\\n**Key milestones in the development of Reinforcement Learning:**\\n\\n- **1950s-1960s:** The origins of RL are linked to research in psychology, particularly Edward Thorndike's Law of Effect, which suggested that behaviors followed by pleasant outcomes are likely to be repeated.\\n- **1960s:** The formal mathematical framework for RL was developed. Richard S. Sutton and Andrew G. Barto are often credited with pioneering work in this area.\\n- **1980s:** The field gained significant momentum with algorithms like Q-learning (introduced by Watkins in 1989) and Temporal Difference learning.\\n- **1990s onward:** RL became central to advancements in artificial intelligence, notably in game playing and robotics.\\n\\n**In summary**, RL as a formal discipline was primarily developed in North America by researchers like Richard Sutton, and it has grown through contributions from computer scientists, psychologists, and mathematicians.\\n\\nWould you like more detailed history or examples?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 243, 'prompt_tokens': 195, 'total_tokens': 438, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0001167, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0001167, 'upstream_inference_prompt_cost': 1.95e-05, 'upstream_inference_completions_cost': 9.72e-05}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-4.1-nano', 'system_fingerprint': None, 'id': 'gen-1768581718-9VKNKEeBkSnj01DLLiQD', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc7af-61de-73e0-b7e3-f6a3a5241f9c-0', usage_metadata={'input_tokens': 195, 'output_tokens': 243, 'total_tokens': 438, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='can you write a simple rl min max algo', additional_kwargs={}, response_metadata={}, id='3a9cd56c-ac76-4d18-90bd-7a94d8e49cda'), AIMessage(content='Certainly! Here\\'s a simple example of a min-max algorithm, which is commonly used in two-player, zero-sum games like Tic-Tac-Toe. Just to clarify, min-max algorithms are not exactly reinforcement learning algorithms, but rather decision-tree search algorithms.\\n\\n### Simple Min-Max Algorithm for Tic-Tac-Toe (Python)\\n\\n```python\\n# Define the board and players\\n# \\'X\\' is maximizing player, \\'O\\' is minimizing player\\n\\ndef is_moves_left(board):\\n    return any(cell == \\'_\\' for row in board for cell in row)\\n\\ndef evaluate(board):\\n    # Check rows\\n    for row in board:\\n        if row.count(\\'X\\') == 3:\\n            return 10\\n        elif row.count(\\'O\\') == 3:\\n            return -10\\n    # Check columns\\n    for col in range(3):\\n        column = [board[row][col] for row in range(3)]\\n        if column.count(\\'X\\') == 3:\\n            return 10\\n        elif column.count(\\'O\\') == 3:\\n            return -10\\n    # Check diagonals\\n    diag1 = [board[i][i] for i in range(3)]\\n    diag2 = [board[i][2 - i] for i in range(3)]\\n    if diag1.count(\\'X\\') == 3 or diag2.count(\\'X\\') == 3:\\n        return 10\\n    if diag1.count(\\'O\\') == 3 or diag2.count(\\'O\\') == 3:\\n        return -10\\n    # No winner\\n    return 0\\n\\ndef minmax(board, depth, is_maximizing):\\n    score = evaluate(board)\\n    if score == 10 or score == -10:\\n        return score\\n    if not is_moves_left(board):\\n        return 0  # Draw\\n\\n    if is_maximizing:\\n        best_score = -float(\\'inf\\')\\n        for i in range(3):\\n            for j in range(3):\\n                if board[i][j] == \\'_\\':\\n                    board[i][j] = \\'X\\'\\n                    current_score = minmax(board, depth + 1, False)\\n                    board[i][j] = \\'_\\'\\n                    best_score = max(best_score, current_score)\\n        return best_score\\n    else:\\n        best_score = float(\\'inf\\')\\n        for i in range(3):\\n            for j in range(3):\\n                if board[i][j] == \\'_\\':\\n                    board[i][j] = \\'O\\'\\n                    current_score = minmax(board, depth + 1, True)\\n                    board[i][j] = \\'_\\'\\n                    best_score = min(best_score, current_score)\\n        return best_score\\n\\n# Example usage:\\ninitial_board = [\\n    [\\'X\\', \\'O\\', \\'X\\'],\\n    [\\'_\\', \\'_\\', \\'_\\'],\\n    [\\'_\\', \\'_\\', \\'_\\']\\n]\\n\\nbest_score = -float(\\'inf\\')\\nbest_move = None\\n\\nfor i in range(3):\\n    for j in range(3):\\n        if initial_board[i][j] == \\'_\\':\\n            initial_board[i][j] = \\'X\\'\\n            move_score = minmax(initial_board, 0, False)\\n            initial_board[i][j] = \\'_\\'\\n            if move_score > best_score:\\n                best_score = move_score\\n                best_move = (i, j)\\n\\nprint(f\"Best move for X: {best_move} with score {best_score}\")\\n```\\n\\n### What this code does:\\n- It evaluates the current state of the Tic-Tac-Toe board.\\n- Uses the min-max algorithm recursively to explore possible future moves.\\n- Finds the optimal move for the maximum player (\\'X\\') assuming the opponent (\\'O\\') also plays optimally.\\n\\n**Note:** This is a simplified example suitable for small games like Tic-Tac-Toe. For larger games, you may need to incorporate optimizations like alpha-beta pruning.\\n\\nWould you like this explained further or adapted for a different game?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 812, 'prompt_tokens': 454, 'total_tokens': 1266, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cost': 0.0003702, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.0003702, 'upstream_inference_prompt_cost': 4.54e-05, 'upstream_inference_completions_cost': 0.0003248}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-4.1-nano', 'system_fingerprint': None, 'id': 'gen-1768581738-hXSq8JPOmKCpQi1ElhAn', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc7af-adf6-7f60-907d-8e0b671fa459-0', usage_metadata={'input_tokens': 454, 'output_tokens': 812, 'total_tokens': 1266, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0f2fa5-673b-61f5-800d-03e2f29f5b72'}}, metadata={'source': 'loop', 'step': 13, 'parents': {}}, created_at='2026-01-16T16:42:25.465378+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0f2fa5-1fc8-6e29-800c-fdc183d4270e'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_state(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56368335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
